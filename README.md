# LLM Fine-Tuning

이 레포는 LLM 파인튜닝 워크플로우를 실습하기 위한 자료실입니다. 천천히 따라하며 배워보세요.

## 설치 및 준비

1. 먼저, 용어집을 다운 받습니다. [AI Hub](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=data&dataSetSn=71614)
    - 160.문화, 게임 콘텐츠 분야 용어 말뭉치\01-1.정식개방데이터\Training\02.라벨링데이터\TL용어.json
    - 용어집의 이름을 dataset.json으로 변경하여 코드와 같은 폴더에 넣어줍니다.

2. `torch`를 다운 받습니다. (먼저 NVIDIA Driver, CUDA, CuDNN이 적절히 깔려있어야 합니다.) [PyTorch](https://pytorch.org/)

3. `transformers`를 다운 받습니다.
    ```bash
    pip install transformers
    ```

## 코드

코드는 크게 두 부분, 전처리와 파인튜닝으로 나뉘어 있습니다.

### 데이터 전처리
- **JSON 데이터 로드**: JSON 파일에서 데이터를 로드합니다.
- **중첩된 JSON 플래트닝**: 중첩된 JSON 구조를 평탄화합니다.
- **데이터셋 분할**: 데이터를 학습 데이터셋과 검증 데이터셋으로 분할합니다.
- **전처리된 데이터셋을 파일로 저장**: 전처리된 데이터를 JSON 파일로 저장합니다.

### 모델 파인튜닝
- **토크나이저 및 모델 로드**: Hugging Face의 토크나이저와 사전 학습된 언어 모델을 로드합니다.
- **데이터셋 토크나이징 및 인코딩**: 데이터셋을 토크나이저를 사용하여 인코딩합니다. (토크나이징도 전처리에 해당하는 부분입니다.)
- **데이터셋을 PyTorch 데이터셋으로 변환**: 인코딩된 데이터를 PyTorch 데이터셋으로 변환합니다.
- **모델 파인튜닝**: Trainer 클래스를 사용하여 모델을 파인튜닝합니다.
- **파인튜닝된 모델 저장**: 파인튜닝된 모델과 토크나이저를 저장합니다.

## 모델 정보

튜닝에 사용한 모델은 `qwen2`입니다. 24년 06월 기준으로 굉장히 핫한 모델로, 중국에서 만들었습니다.

다국어 모델 중에서 가장 높은 성능을 갖고 있고, 0.5b 파라미터짜리 모델도 나와있어 4090 한 대로도 파인튜닝이 가능합니다. (VRAM이 충분하다면 LLaMA3 기반의 블라썸(서울 과기대)을 추천드립니다.)

모델이 학습할 때 사용된 토크나이저를 그대로 사용해야 파인튜닝도 효과적으로 이루어지니 주의하세요.

## 기타 추천 사항

이 레포에서는 Hugging Face로 파인튜닝을 진행했으나 아홀로틀(Axolotl)로 진행하는 게 더 바람직합니다.

더 쉬운 걸 찾고 있다면 코드 없이도 진행 가능한 라마-팩토리나 우바부가를 추천드리고, 돈이 정말 없는데 파인튜닝이 꼭 해보고 싶다면 Unsloth을 추천드립니다.

